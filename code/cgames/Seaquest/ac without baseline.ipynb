{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efa76703-4375-44be-89b9-3cafe7d794a7",
   "metadata": {},
   "source": [
    "## Step 1: Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb1432-4588-4f7a-ab47-6453b3dca52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9618ca48-00ba-4269-a055-512ba3d0a152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "class A2CAgent():\n",
    "    def __init__(self, input_shape, action_size, seed, device, gamma, alpha, beta, update_every, net):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        Params\n",
    "        ======\n",
    "            input_shape (tuple): dimension of each state (C, H, W)\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "            device(string): Use Gpu or CPU\n",
    "            gamma (float): discount factor\n",
    "            alpha (float): Actor learning rate\n",
    "            beta (float): Critic learning rate \n",
    "            update_every (int): how often to update the network\n",
    "            actor_m(Model): Pytorch Actor Model\n",
    "            critic_m(Model): PyTorch Critic Model\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.update_every = update_every\n",
    "\n",
    "        # Actor-Network\n",
    "        self.net = net(input_shape, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.alpha)\n",
    "\n",
    "        # Critic-Network\n",
    "        #self.critic_net = critic_m(input_shape).to(self.device)\n",
    "        #self.critic_optimizer = optim.SGD(self.critic_net.parameters(), lr=self.beta)\n",
    "\n",
    "        # Memory\n",
    "        self.log_probs = []\n",
    "        self.values    = []\n",
    "        self.rewards   = []\n",
    "        self.masks     = []\n",
    "        self.entropies = []\n",
    "\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, log_prob, entropy, reward, done, next_state):\n",
    "\n",
    "        state = torch.from_numpy(state).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        \n",
    "        _,value = self.net(state)\n",
    "        \n",
    "        # Save experience in  memory\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.values.append(value)\n",
    "        self.rewards.append(torch.from_numpy(np.array([reward])).to(self.device))\n",
    "        self.masks.append(torch.from_numpy(np.array([1 - done])).to(self.device))\n",
    "        self.entropies.append(entropy)\n",
    "\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0:\n",
    "            self.learn(next_state)\n",
    "            self.reset_memory()\n",
    "                \n",
    "    def act(self, state):\n",
    "        \"\"\"Returns action, log_prob, entropy for given state as per current policy.\"\"\"\n",
    "        \n",
    "        state = torch.from_numpy(state).unsqueeze(0).to(self.device)\n",
    "        action_probs,_ = self.net(state)\n",
    "\n",
    "        action = action_probs.sample()\n",
    "        log_prob = action_probs.log_prob(action)\n",
    "        entropy = action_probs.entropy().mean()\n",
    "\n",
    "        return action.item(), log_prob, entropy\n",
    "    def learn(self, next_state):\n",
    "        next_state = torch.from_numpy(next_state).unsqueeze(0).to(self.device)\n",
    "        _,next_value = self.net(next_state)\n",
    "        \n",
    "        returns = self.compute_returns(next_value, self.gamma)\n",
    "\n",
    "        log_probs = torch.cat(self.log_probs)\n",
    "        returns   = torch.cat(returns).detach()\n",
    "        values    = torch.cat(self.values)\n",
    "\n",
    "        advantage = returns \n",
    "        #advantage = returns\n",
    "        \n",
    "        actor_loss  = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        loss = actor_loss + 0.5 * critic_loss - 0.001 * sum(self.entropies)\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #actor_loss.backward()\n",
    "        #critic_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def reset_memory(self):\n",
    "        del self.log_probs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.values[:]\n",
    "        del self.masks[:]\n",
    "        del self.entropies[:]\n",
    "\n",
    "    def compute_returns(self, next_value, gamma=0.99):\n",
    "        R = next_value\n",
    "        returns = []\n",
    "        for step in reversed(range(len(self.rewards))):\n",
    "            R = self.rewards[step] + gamma * R * self.masks[step]\n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "    def save_model(self,actor_result_path,critic_result_path):\n",
    "        torch.save({\"state_dict\": self.actor_net.state_dict()}, actor_result_path)\n",
    "        torch.save({\"state_dict\": self.critic_net.state_dict()}, critic_result_path)\n",
    "    def return_loss(self):\n",
    "        return self.los, self.a_l, self.c_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0f1b43-7e8e-47db-98f8-a6aec54aa14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994f98de-9c6c-4216-a2c2-ea4ecf622086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class ActorCnn(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ActorCnn, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU())\n",
    "        self.actor_fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.critic_fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        actor = self.actor_fc(x)\n",
    "        critic = self.critic_fc(x)\n",
    "        actor = Categorical(actor)\n",
    "        \n",
    "        return actor, critic\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fee305-6120-454c-a989-c516b93ca75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def preprocess_frame(screen, exclude, output):\n",
    "    \"\"\"Preprocess Image.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            screen (array): RGB Image\n",
    "            exclude (tuple): Section to be croped (UP, RIGHT, DOWN, LEFT)\n",
    "            output (int): Size of output image\n",
    "        \"\"\"\n",
    "    # TConver image to gray scale\n",
    "    screen = cv2.cvtColor(screen, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    #Crop screen[Up: Down, Left: right] \n",
    "    screen = screen[exclude[0]:exclude[2], exclude[3]:exclude[1]]\n",
    "    \n",
    "    # Convert to float, and normalized\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    \n",
    "    # Resize image to 84 * 84\n",
    "    screen = cv2.resize(screen, (output, output), interpolation = cv2.INTER_AREA)\n",
    "    return screen\n",
    "\n",
    "def stack_frame(stacked_frames, frame, is_new):\n",
    "    \"\"\"Stacking Frames.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            stacked_frames (array): Four Channel Stacked Frame\n",
    "            frame: Preprocessed Frame to be added\n",
    "            is_new: Is the state First\n",
    "        \"\"\"\n",
    "    if is_new:\n",
    "        stacked_frames = np.stack(arrays=[frame, frame, frame, frame])\n",
    "        stacked_frames = stacked_frames\n",
    "    else:\n",
    "        stacked_frames[0] = stacked_frames[1]\n",
    "        stacked_frames[1] = stacked_frames[2]\n",
    "        stacked_frames[2] = stacked_frames[3]\n",
    "        stacked_frames[3] = frame\n",
    "    \n",
    "    return stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caaafdf-ea3e-43fa-bfdd-d5f921086d40",
   "metadata": {},
   "source": [
    "## Step 2: Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fea46c0-e18d-41b5-861e-6a84b465e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Seaquest-v5\")\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22847452-4604-4a55-b64c-be1b6383d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde99054-2155-4859-917b-2d50459baec6",
   "metadata": {},
   "source": [
    "## Step 3: Viewing our Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77c1e7b-3efe-4b07-962f-5badbd52dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The size of frame is: \", env.observation_space.shape)\n",
    "print(\"No. of Actions: \", env.action_space.n)\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(env.reset())\n",
    "plt.title('Original Frame')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0211fb-2a49-4969-a01e-31f672e9ebcd",
   "metadata": {},
   "source": [
    "### Execute the code cell below to play Pong with a random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cdf471-8c97-4ee0-846c-f4b3a9d0f13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_play():\n",
    "    score = 0\n",
    "    env.reset()\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        score += reward\n",
    "        if done:\n",
    "            env.close()\n",
    "            print(\"Your Score at end of game is: \", score)\n",
    "            break\n",
    "random_play()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e375776-6409-48ad-939f-c59250c0aabe",
   "metadata": {},
   "source": [
    "## Step 4:Preprocessing Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7270c8e0-c254-452f-9749-ef04370bffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(preprocess_frame(env.reset(), (8, -12, -12, 4), 84), cmap=\"gray\")\n",
    "plt.title('Pre Processed image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be74a89-3dcc-4ddd-bdf9-bb9dc4f1e777",
   "metadata": {},
   "source": [
    "## Step 5: Stacking Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b45b872-55f3-4cc9-8c33-da9a20471dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(frames, state, is_new=False):\n",
    "    frame = preprocess_frame(state, (8, -12, -12, 4), 84)\n",
    "    frames = stack_frame(frames, frame, is_new)\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc1e895-7d58-4b7d-b388-394b81b4b55a",
   "metadata": {},
   "source": [
    "## Step 6: Creating our Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef36624-7df0-435c-bf8c-a5b56df2a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (4, 84, 84)\n",
    "ACTION_SIZE = env.action_space.n\n",
    "SEED = 0\n",
    "GAMMA = 0.99           # discount factor\n",
    "ALPHA= 0.0001          # Actor learning rate 0.001\n",
    "BETA = 0.0005          # Critic learning rate0.005\n",
    "UPDATE_EVERY = 100     # how often to update the network \n",
    "\n",
    "agent = A2CAgent(INPUT_SHAPE, ACTION_SIZE, SEED, device, GAMMA, ALPHA, BETA, UPDATE_EVERY, ActorCnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13342145-6ed9-47e3-88f7-0c11a031143d",
   "metadata": {},
   "source": [
    "## Step 7: Watching untrained agent play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f3bd3a-4a92-4cd6-af2f-bc03e1e07298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch an untrained agent\n",
    "state = stack_frames(None, env.reset(), True) \n",
    "for j in range(200):\n",
    "    action, _, _ = agent.act(state)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = stack_frames(state, next_state, False)\n",
    "    if done:\n",
    "        break \n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9af61c-3ae3-4608-ba08-a166eefa8a95",
   "metadata": {},
   "source": [
    "## Step 8: Loading Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62649f04-7cb0-4614-a1ab-33347124f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "scores = []\n",
    "scores_window = deque(maxlen=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa25bd0-d524-4945-bf6b-3ae22332aef2",
   "metadata": {},
   "source": [
    "## Step 9: Train the Agent with DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa39ab-f689-4a76-84e0-4dad407a6234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_episodes=1000):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "    \"\"\"\n",
    "    steps = 0 \n",
    "    for i_episode in range(start_epoch + 1, n_episodes+1):\n",
    "        state = stack_frames(None, env.reset(), True)\n",
    "        score = 0\n",
    "        while True:\n",
    "            steps += 1\n",
    "            action, log_prob, entropy = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            next_state = stack_frames(state, next_state, False)\n",
    "            agent.step(state, log_prob, entropy, reward, done, next_state)\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        clear_output(True)\n",
    "        fig = plt.figure(figsize=(15,8))\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.plot(np.arange(len(scores)), scores)\n",
    "        plt.ylabel('Score')\n",
    "        plt.xlabel('Episode #')\n",
    "        plt.show()\n",
    "        print('\\rEpisode {}\\tSteps {}\\tAverage Score: {:.2f}'.format(i_episode, steps,scores[-1]), end=\"\")\n",
    "        print('\\n')\n",
    "        \n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbbac19-5cd5-4ea1-b425-6b19c41402b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = train(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc201f0a-d89e-401d-97e5-7f042696a20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,1,1)\n",
    "pd.Series(scores).ewm(span=100).mean().plot()\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
