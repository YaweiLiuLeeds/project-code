{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f574c63-ba39-450e-9e20-33d3c3c1ec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd \n",
    "from torch.distributions import Categorical\n",
    "import cv2\n",
    "import time\n",
    "import gym\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab266cc-ead3-45ab-8baa-11051961d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent():\n",
    "    def __init__(self, input_shape, action_size, seed, device, gamma, alpha, beta, tau, update_every, batch_size, ppo_epoch, clip_param, actor_m):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        Params\n",
    "        ======\n",
    "            input_shape (tuple): dimension of each state (C, H, W)\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "            device(string): Use Gpu or CPU\n",
    "            gamma (float): discount factor\n",
    "            alpha (float): Actor learning rate\n",
    "            beta (float): Critic learning rate \n",
    "            tau (float): Tau Value\n",
    "            update_every: How often to update network\n",
    "            batch_size (int): Mini Batch size to be used every epoch \n",
    "            ppo_epoch(int): Total No epoch for ppo\n",
    "            clip_param(float): Clip Paramter\n",
    "            actor_m(Model): Pytorch Actor Model\n",
    "            critic_m(Model): PyTorch Critic Model\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.tau = tau\n",
    "        self.update_every = update_every\n",
    "        self.batch_size = batch_size\n",
    "        self.ppo_epoch = ppo_epoch\n",
    "        self.clip_param = clip_param\n",
    "\n",
    "        # Actor-Network\n",
    "        self.net = actor_m(input_shape, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.alpha)\n",
    "\n",
    "        # Memory\n",
    "        self.log_probs = []\n",
    "        self.values    = []\n",
    "        self.states    = []\n",
    "        self.actions   = []\n",
    "        self.rewards   = []\n",
    "        self.masks     = []\n",
    "        self.entropies = []\n",
    "\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, value, log_prob, reward, done, next_state):\n",
    "        \n",
    "        # Save experience in  memory\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.values.append(value)\n",
    "        self.states.append(torch.from_numpy(state).unsqueeze(0).to(self.device))\n",
    "        self.rewards.append(torch.from_numpy(np.array([reward])).to(self.device))\n",
    "        self.actions.append(torch.from_numpy(np.array([action])).to(self.device))\n",
    "        self.masks.append(torch.from_numpy(np.array([1 - done])).to(self.device))\n",
    "\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "\n",
    "        if self.t_step == 0:\n",
    "            self.learn(next_state)\n",
    "            self.reset_memory()\n",
    "                \n",
    "    def act(self, state):\n",
    "        \"\"\"Returns action, log_prob, value for given state as per current policy.\"\"\"\n",
    "        \n",
    "        state = torch.from_numpy(state).unsqueeze(0).to(self.device)\n",
    "        action_probs,_ = self.net(state)\n",
    "        _,value = self.net(state)\n",
    "\n",
    "        action = action_probs.sample()\n",
    "        log_prob = action_probs.log_prob(action)\n",
    "\n",
    "        return action.item(), log_prob, value\n",
    "        \n",
    "    def learn(self, next_state):\n",
    "        next_state = torch.from_numpy(next_state).unsqueeze(0).to(self.device)\n",
    "        _,next_value = self.net(next_state)\n",
    "\n",
    "        returns        = torch.cat(self.compute_gae(next_value)).detach()\n",
    "        self.log_probs = torch.cat(self.log_probs).detach()\n",
    "        self.values    = torch.cat(self.values).detach()\n",
    "        self.states    = torch.cat(self.states)\n",
    "        self.actions   = torch.cat(self.actions)\n",
    "        advantages     = returns - self.values\n",
    "\n",
    "        for _ in range(self.ppo_epoch):\n",
    "            for state, action, old_log_probs, return_, advantage in self.ppo_iter(returns, advantages):\n",
    "\n",
    "                dist,_ = self.net(state)\n",
    "                _,value = self.net(state)\n",
    "\n",
    "                entropy = dist.entropy().mean()\n",
    "                new_log_probs = dist.log_prob(action)\n",
    "\n",
    "                ratio = (new_log_probs - old_log_probs).exp()\n",
    "                surr1 = ratio * advantage\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * advantage\n",
    "\n",
    "                actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "                critic_loss = (return_ - value).pow(2).mean()\n",
    "                \n",
    "                loss = 0.5 * actor_loss + 0.5 * critic_loss - 0.001 * entropy\n",
    "\n",
    "                # Minimize the loss\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        self.reset_memory()\n",
    "\n",
    "    \n",
    "    def ppo_iter(self, returns, advantage):\n",
    "        memory_size = self.states.size(0)\n",
    "        for _ in range(memory_size // self.batch_size):\n",
    "            rand_ids = np.random.randint(0, memory_size, self.batch_size)\n",
    "            yield self.states[rand_ids, :], self.actions[rand_ids], self.log_probs[rand_ids], returns[rand_ids, :], advantage[rand_ids, :]\n",
    "\n",
    "    def reset_memory(self):\n",
    "        self.log_probs = []\n",
    "        self.values    = []\n",
    "        self.states    = []\n",
    "        self.actions   = []\n",
    "        self.rewards   = []\n",
    "        self.masks     = []\n",
    "        self.entropies = []\n",
    "\n",
    "    def compute_gae(self, next_value):\n",
    "        gae = 0\n",
    "        returns = []\n",
    "        values = self.values + [next_value]\n",
    "        for step in reversed(range(len(self.rewards))):\n",
    "            delta = self.rewards[step] + self.gamma * values[step + 1] * self.masks[step] - values[step]\n",
    "            gae = delta + self.gamma * self.tau * self.masks[step] * gae\n",
    "            returns.insert(0, gae + values[step])\n",
    "        return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bba440-6094-4ef6-a7c4-d026d6828c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class ActorCnn(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(ActorCnn, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.actor_fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.critic_fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        actor = self.actor_fc(x)\n",
    "        critic = self.critic_fc(x)\n",
    "        actor = Categorical(actor)\n",
    "        \n",
    "        return actor, critic\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef5d3fd-26d6-4f68-95cb-a40ac6f5f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(screen, exclude, output):\n",
    "    \"\"\"Preprocess Image.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            screen (array): RGB Image\n",
    "            exclude (tuple): Section to be croped (UP, RIGHT, DOWN, LEFT)\n",
    "            output (int): Size of output image\n",
    "        \"\"\"\n",
    "    # TConver image to gray scale\n",
    "    screen = cv2.cvtColor(screen, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    #Crop screen[Up: Down, Left: right] \n",
    "    screen = screen[exclude[0]:exclude[2], exclude[3]:exclude[1]]\n",
    "    \n",
    "    # Convert to float, and normalized\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    \n",
    "    # Resize image to 84 * 84\n",
    "    screen = cv2.resize(screen, (output, output), interpolation = cv2.INTER_AREA)\n",
    "    return screen\n",
    "\n",
    "def stack_frame(stacked_frames, frame, is_new):\n",
    "    \"\"\"Stacking Frames.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            stacked_frames (array): Four Channel Stacked Frame\n",
    "            frame: Preprocessed Frame to be added\n",
    "            is_new: Is the state First\n",
    "        \"\"\"\n",
    "    if is_new:\n",
    "        stacked_frames = np.stack(arrays=[frame, frame, frame, frame])\n",
    "        stacked_frames = stacked_frames\n",
    "    else:\n",
    "        stacked_frames[0] = stacked_frames[1]\n",
    "        stacked_frames[1] = stacked_frames[2]\n",
    "        stacked_frames[2] = stacked_frames[3]\n",
    "        stacked_frames[3] = frame\n",
    "    \n",
    "    return stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689b8b8c-68ad-438e-b118-6d516ce00b63",
   "metadata": {},
   "source": [
    "## Create our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5ba7de-c1b4-4d00-9631-7373546fa1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/SpaceInvaders-v5\")\n",
    "env.seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ebaa73-167a-4b89-b763-c678856906d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The size of frame is: \", env.observation_space.shape)\n",
    "print(\"No. of Actions: \", env.action_space.n)\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(env.reset())\n",
    "plt.title('Original Frame')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d021783d-0f1d-4f7f-8ff7-092a98c6d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(preprocess_frame(env.reset(), (8, -12, -12, 4), 84), cmap=\"gray\")\n",
    "plt.title('Pre Processed image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e8d44-2e09-45ad-b88b-2b96e453d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(frames, state, is_new=False):\n",
    "    frame = preprocess_frame(state, (8, -12, -12, 4), 84)\n",
    "    frames = stack_frame(frames, frame, is_new)\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7369bb-f9c9-4340-ae46-89e8b64d9d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (4, 84, 84)\n",
    "ACTION_SIZE = env.action_space.n\n",
    "SEED = 0\n",
    "GAMMA = 0.99           # discount factor\n",
    "ALPHA= 0.0001          # Actor learning rate\n",
    "BETA = 0.0001          # Critic learning rate\n",
    "TAU = 0.95\n",
    "BATCH_SIZE = 32\n",
    "PPO_EPOCH = 5\n",
    "CLIP_PARAM = 0.2\n",
    "UPDATE_EVERY = 1000    # how often to update the network \n",
    "\n",
    "\n",
    "agent = PPOAgent(INPUT_SHAPE, ACTION_SIZE, SEED, device, GAMMA, ALPHA, BETA, TAU, UPDATE_EVERY, BATCH_SIZE, PPO_EPOCH, CLIP_PARAM, ActorCnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea10625e-71ef-4dec-80ab-7cc3adfe3ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "scores = []\n",
    "scores_window = deque(maxlen=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879dbd25-79e8-4329-8ed2-dc15ad0fa97b",
   "metadata": {},
   "source": [
    "## Train the Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da9db1a-5605-4ba5-bd53-f93d7c085cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_episodes=1000):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "    \"\"\"\n",
    "    for i_episode in range(start_epoch + 1, n_episodes+1):\n",
    "        state = stack_frames(None, env.reset(), True)\n",
    "        score = 0\n",
    "        steps = 0\n",
    "        while True:\n",
    "            steps += 1\n",
    "            action, log_prob, value = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            next_state = stack_frames(state, next_state, False)\n",
    "            agent.step(state, action, value, log_prob, reward, done, next_state)\n",
    "            scores_window.append(score)\n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                state = next_state\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(np.mean(scores_window))              # save most recent score\n",
    "        \n",
    "        clear_output(True)\n",
    "        fig = plt.figure(figsize=(15,8))\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.plot(np.arange(len(scores)), scores)\n",
    "        plt.ylabel('Score')\n",
    "        plt.xlabel('Episode #')\n",
    "        plt.show()\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "    #agent.save_model('model/PPO_actor_result_path.pt','model/PPO_critic_result_path.pt')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7a363e-bb8c-4e97-9471-3a469a04e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = train(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c3d764-4862-4f2f-afc1-f8d95561ab92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a51a83a-1bd1-4155-b884-b9ca4f7500ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,1,1)\n",
    "#pd.Series(scores[:1000]).plot(label='train_score')\n",
    "pd.Series(scores[0:2000]).ewm(span=200).mean().plot(label='A2C score')\n",
    "#pd.Series(x).ewm(span=200).mean().plot(label='AC score')\n",
    "#plt.plot(np.arange(len(a[:800])), a[:800],color='green',label='PPO_scores')\n",
    "#pd.Series(b[:800]).ewm(span=100).mean().plot(label='PPO_without_baseline_score',color='red')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode ')\n",
    "plt.title('AC And A2C on SpaceInvaders-V5')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18eab55-422f-4077-83c5-df52184d1088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e9956-1335-443f-9b56-6ce88867ad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=np.array(scores)\n",
    "np.save('ppo_without_baseline_BeamRider_2000.npy',a)   # 保存为.npy格式\n",
    "# 读取\n",
    "#a=np.load('a.npy')\n",
    "#a=a.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc6588f-759d-4404-b8d8-32e0b693a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores = []\n",
    "for i in range(len(scores)):\n",
    "    mean_scores.append(np.mean(scores_window))\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores[:100])), scores[:100])\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.plot(np.arange(len(scores[:100])),mean_scores[:100],color = 'red')\n",
    "plt.legend(['Score','AverageScore'], loc='upper left')\n",
    "plt.title('PPO,average score : {}'.format(mean_scores[0]))\n",
    "plt.show()\n",
    "print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(50, np.mean(scores_window)), end=\"\")\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34f48d2-c26c-4100-b53f-ec9a09d23dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores = []\n",
    "for i in range(len(scores)):\n",
    "    mean_scores.append(np.mean(scores_window))\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len([0,100,200,300,400,500])), scores[:50])\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.plot(np.arange(len(scores[:50])),mean_scores[:50],color = 'red')\n",
    "plt.legend(['Score','AverageScore'], loc='upper left')\n",
    "plt.title('PPO,average score : {}'.format(mean_scores[0]))\n",
    "plt.show()\n",
    "print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(50, np.mean(scores_window)), end=\"\")\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb35322-1370-4b54-b216-cd7168aaa46a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
